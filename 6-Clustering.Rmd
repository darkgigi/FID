```{r}
library(tidyverse)
library(cluster)
library(factoextra)
library(caret)
library(ggplot2)
library(rgl)

# 1. Lectura de dataset
df=read.table("data/student-por.csv",sep=",",header=TRUE)

# Convertir columnas necesarias a factor
cols_to_factor <- c("school", "sex", "address", "famsize", "Pstatus", "Medu", "Fedu", "Mjob", 
                    "Fjob", "reason", "guardian", "traveltime", "studytime" ,"famrel"
                    , "freetime", "goout", "health")

df[cols_to_factor] <- lapply(df[cols_to_factor], as.factor)
cols_to_boolean <- c("schoolsup", "famsup", "paid", "activities", "nursery", "higher", "internet", "romantic")
df[cols_to_boolean] <- lapply(df[cols_to_boolean], function(x) x == "yes")
data <- df

# 2. Seleccionar las variables a tener en cuenta en la clusterización. Hemos elegido las variables que hemos considerado más interesantes relativas a las notas obtenidas y al consumo de alcohol de los alumnos
relevant_variables <- data %>% select(G1, G2, G3, Dalc, Walc)

# 3. Convertir variables categóricas a variables numéricas utilizando one-hot encoding
dummies <- dummyVars(~ ., data = relevant_variables)
variables_encoded <- predict(dummies, newdata = relevant_variables)

# 4. Escalar las variables
variables_scaled <- scale(variables_encoded)

# 4.1. Calcular Silhouette para determinar el número óptimo de clusters
silhouette_scores <- function(k) {
  km <- kmeans(variables_scaled, centers = k, nstart = 25)
  ss <- silhouette(km$cluster, dist(variables_scaled))
  mean(ss[, 3])
}

# 5. Calcular el índice de Silhouette para diferentes números de clusters
k_values <- 2:10
avg_silhouette <- sapply(k_values, silhouette_scores)

# 6. Graficar el índice de Silhouette
plot(k_values, avg_silhouette, type = "b", pch = 19, frame = FALSE, 
     xlab = "Número de clusters K", ylab = "Promedio del índice de Silhouette",
     main = "Promedio del índice de Silhouette para K")

# 7. Determinar el número óptimo de clusters (Método del codo)
fviz_nbclust(variables_scaled, FUNcluster = function(x, k) kmeans(x, centers = k, nstart = 25), method = "wss") +
  geom_vline(xintercept = 3, linetype = 2) +
  labs(title = "Método del Codo para Determinar el Número de Clústeres")

# 8. Aplicar K-means con k = 3
set.seed(42) # Para reproducibilidad
kmeans_result <- kmeans(variables_scaled, centers = 3, nstart = 25)

# 9. Agregar el resultado al dataset original
data$cluster <- as.factor(kmeans_result$cluster)

# 10. Visualización de clusters en las dimensiones originales (ND)
fviz_cluster(kmeans_result, data = variables_scaled, 
             geom = "point", 
             ellipse.type = "convex", 
             ggtheme = theme_minimal()) +
  labs(title = "Resultados del Clustering K-means en Dimensiones Originales")

# 11. Reducir la dimensionalidad a 3 dimensiones utilizando PCA
pca_result <- prcomp(variables_scaled, center = TRUE, scale. = TRUE)
pca_data <- data.frame(pca_result$x[, 1:3], cluster = as.factor(kmeans_result$cluster))

# 12. Graficar los resultados en 3D utilizando rgl
plot3d(pca_data$PC1, pca_data$PC2, pca_data$PC3, col = as.numeric(pca_data$cluster), type = "s", size = 1)
title3d("Clustering K-means con PCA", xlab = "Componente Principal 1", ylab = "Componente Principal 2", zlab = "Componente Principal 3")

# 13. Análisis descriptivo de los clusters
data %>% group_by(cluster) %>%
  summarise(across(c(G1, G2, G3, Dalc, Walc), mean, na.rm = TRUE))

# 14. Variables más representativas 
# Dim1
fviz_contrib(pca_result, choice = "var", axes = 1, top = 5)
# Dim2
fviz_contrib(pca_result, choice = "var", axes = 2, top = 5)
```
